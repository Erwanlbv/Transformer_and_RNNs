{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tqdm \n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation de wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merwanlbv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"PureEmbdeding with Sweep and Tensorflow\"\n"
     ]
    }
   ],
   "source": [
    "%env \"WANDB_NOTEBOOK_NAME\" \"PureEmbdeding with Sweep and Tensorflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/erwan/Programmes/Stage/dlexperiments/Erwan/Text_Classification/datasets/Tweeter/french_tweets.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "shuff_df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,\n",
       " 1    515\n",
       " 0    485\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_size = 1000\n",
    "small_df = shuff_df[:df_size]\n",
    "len(small_df), small_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction du tokenizer & des ensembles de données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer_and_datasets(df, config):\n",
    "    print(\"------------\")\n",
    "    \n",
    "    def create_ds(df, size):\n",
    "        shuffled_df = shuffle(df)[:size]\n",
    "        text_seq = shuffled_df['text']\n",
    "        target_seq = shuffled_df['label']\n",
    "        ds = tf.data.Dataset.from_tensor_slices((text_seq, target_seq))\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "    ds = create_ds(df, config.global_ds_size)\n",
    "    ds_size = len(ds)    \n",
    "    print(f\" Ensemble de données créé, taille : {ds_size}\")   \n",
    "    print(\"------------\")\n",
    "\n",
    "\n",
    "    train_size = int(config.train_split * ds_size)\n",
    "    val_size = int(config.val_split * ds_size)\n",
    "    print(f\" Taille des ensembles de données : {train_size}, {val_size}\")\n",
    "    \n",
    "    ds.shuffle(1)\n",
    "\n",
    "    str_train_ds = ds.take(train_size).batch(config.batch_size)\n",
    "    str_val_ds = ds.skip(train_size).take(val_size).batch(config.batch_size)\n",
    "    str_test_ds = ds.skip(train_size + val_size).batch(config.batch_size)\n",
    "\n",
    "    print(\"Fin du chargement des bases de données\")\n",
    "    print(len(str_train_ds) * config.batch_size, len(str_val_ds) * config.batch_size, len(str_test_ds) * config.batch_size)\n",
    "    print(\"------------\")\n",
    "\n",
    "    tokenizer_layer = tf.keras.layers.TextVectorization(\n",
    "        standardize='lower_and_strip_punctuation',\n",
    "        split='whitespace',\n",
    "        max_tokens=config.vocab_size,\n",
    "        output_sequence_length=config.max_length,\n",
    "    )\n",
    "\n",
    "    # On entraine le tokenizer sur l'ensemble de données d'entraînement\n",
    "    tokenizer_layer.adapt(str_train_ds.map(lambda text, label: text))\n",
    "    print(\"Fin de l'entraînement du tokenizer\")\n",
    "    print(\"------------\")\n",
    "\n",
    "\n",
    "    # Préparation des ensembles de données : \n",
    "    def tokenize_text(text, label):\n",
    "        text = tf.expand_dims(text, -1) # Explication -1 -> tf.data.Dataset -> \"map\"\n",
    "        res = tokenizer_layer(text)\n",
    "        \n",
    "        return res, label\n",
    "\n",
    "    train_ds = str_train_ds.map(tokenize_text)\n",
    "    val_ds = str_val_ds.map(tokenize_text)\n",
    "    test_ds = str_test_ds.map(tokenize_text)\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    print(\"Fin de la préparation des bases de donneés\")\n",
    "    print(len(train_ds) * config.batch_size + len(val_ds) * config.batch_size + len(test_ds) * config.batch_size)\n",
    "    print(\"-----------\")\n",
    "\n",
    "    return tokenizer_layer, train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(config):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(config.vocab_size, config.embedding_dim),\n",
    "    tf.keras.layers.Dropout(config.drop1),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dropout(config.drop2),\n",
    "    tf.keras.layers.Dense(1),  # activation=None de base, la sortie n'est donc pas normalisée\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_callback(config, name):\n",
    "\n",
    "    log_dir = config.callbacks_log_dir\n",
    "\n",
    "    log_dir = log_dir + name\n",
    "    log_model = log_dir + name + '/models'\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir, \n",
    "        histogram_freq=1\n",
    "    )\n",
    "\n",
    "    model_checkpoint_callbacks = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=log_model,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_binary_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        initial_value_threshold=0.60,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_binary_accuracy',\n",
    "        min_delta=config.early_stopping_min_delta,\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        mode='auto',\n",
    "        baseline=None,\n",
    "        restore_best_weights=False\n",
    "    )\n",
    "\n",
    "    CALLBACKS = [tensorboard_callback, model_checkpoint_callbacks, early_stopping_callbacks]\n",
    "\n",
    "    return CALLBACKS    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration de Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'val_binary_acc'},\n",
      " 'parameters': {'batch_size': {'value': 50},\n",
      "                'callbacks_log_dir': {'value': 'logs/run/'},\n",
      "                'drop1': {'values': [0.2, 0.3, 0.4]},\n",
      "                'drop2': {'values': [0.2, 0.3, 0.4]},\n",
      "                'early_stopping_min_delta': {'value': 0.001},\n",
      "                'embedding_dim': {'values': [100, 200, 500]},\n",
      "                'epochs': {'value': 5},\n",
      "                'global_ds_size': {'value': 1000},\n",
      "                'lr': {'value': '8e-4'},\n",
      "                'max_length': {'value': 200},\n",
      "                'train_split': {'value': 0.7},\n",
      "                'val_split': {'value': 0.15},\n",
      "                'vocab_size': {'values': [15000, 20000, 30000]}}}\n"
     ]
    }
   ],
   "source": [
    "sweep_config_path = r'sweep_config.yaml'\n",
    "\n",
    "with open(sweep_config_path) as file:\n",
    "    sweep_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_train():\n",
    "\n",
    "  with wandb.init():\n",
    "\n",
    "    wandb.config.architecture_name = \"PureEmbedding\"\n",
    "    wandb.config.dataset_name = \"tweeter-fr\"\n",
    "\n",
    "    tokenizer, train_ds, val_ds, test_ds = build_tokenizer_and_datasets(\n",
    "      df=df,  \n",
    "      config=wandb.config\n",
    "    )\n",
    "\n",
    "    callbacks = build_callback(wandb.config, wandb.run.name)\n",
    "\n",
    "    model = build_model(wandb.config)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=wandb.config.lr)\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    model.compile(\n",
    "      optimizer=optimizer,\n",
    "      loss=loss_fn,\n",
    "      metrics=[acc_metric]\n",
    "    )\n",
    "\n",
    "    print(f\"Entraînement lancé \\n - Nom : {wandb.run.name} \\n - Configuration : {wandb.config} \\n\\n\")\n",
    "\n",
    "    history = model.fit(\n",
    "      train_ds,\n",
    "      validation_data=val_ds,\n",
    "      epochs=wandb.config.epochs, # Est capable de le trouver tout seul\n",
    "      callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    # On enregistre la meilleure précision de l'epoch sur l'ensemble de validation\n",
    "    print(history.history)\n",
    "    print(max(history.history['val_binary_accuracy']))\n",
    "    wandb.config.best_val_bin_acc = max(history.history['val_binary_accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: xn3q70sq\n",
      "Sweep URL: https://wandb.ai/erwanlbv/Remote-PureEmb-tf/sweeps/xn3q70sq\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Remote-PureEmb-tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ccsal6db with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcallbacks_log_dir: logs/run/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop1: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop2: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_min_delta: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tglobal_ds_size: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_split: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_split: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvocab_size: 20000\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/erwan/Programmes/Stage/dlexperiments/Erwan/Tweeter_Dataset/wandb/run-20220803_165743-ccsal6db</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/ccsal6db\" target=\"_blank\">summer-sweep-5</a></strong> to <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/sweeps/xn3q70sq\" target=\"_blank\">https://wandb.ai/erwanlbv/Remote-PureEmb-tf/sweeps/xn3q70sq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      " Ensemble de données créé, taille : 1000\n",
      "------------\n",
      " Taille des ensembles de données : 700, 150\n",
      "Fin du chargement des bases de données\n",
      "700 150 150\n",
      "------------\n",
      "Fin de l'entraînement du tokenizer\n",
      "------------\n",
      "Fin de la préparation des bases de donneés\n",
      "1000\n",
      "-----------\n",
      "Entraînement lancé \n",
      " - Nom : summer-sweep-5 \n",
      " - Configuration : {'batch_size': 50, 'callbacks_log_dir': 'logs/run/', 'drop1': 0.4, 'drop2': 0.2, 'early_stopping_min_delta': 0.001, 'embedding_dim': 500, 'epochs': 5, 'global_ds_size': 1000, 'lr': 0.0008, 'max_length': 200, 'train_split': 0.7, 'val_split': 0.15, 'vocab_size': 20000, 'architecture_name': 'PureEmbedding', 'dataset_name': 'tweeter-fr'} \n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6931 - binary_accuracy: 0.5257\n",
      "Epoch 1: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 5s 348ms/step - loss: 0.6931 - binary_accuracy: 0.5257 - val_loss: 0.6915 - val_binary_accuracy: 0.5267\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6912 - binary_accuracy: 0.5257\n",
      "Epoch 2: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 3s 220ms/step - loss: 0.6912 - binary_accuracy: 0.5257 - val_loss: 0.6912 - val_binary_accuracy: 0.5267\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6902 - binary_accuracy: 0.5257\n",
      "Epoch 3: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 2s 185ms/step - loss: 0.6902 - binary_accuracy: 0.5257 - val_loss: 0.6910 - val_binary_accuracy: 0.5267\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6889 - binary_accuracy: 0.5257\n",
      "Epoch 4: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 2s 175ms/step - loss: 0.6889 - binary_accuracy: 0.5257 - val_loss: 0.6907 - val_binary_accuracy: 0.5267\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6883 - binary_accuracy: 0.5257\n",
      "Epoch 5: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 2s 177ms/step - loss: 0.6883 - binary_accuracy: 0.5257 - val_loss: 0.6903 - val_binary_accuracy: 0.5267\n",
      "{'loss': [0.6930726170539856, 0.69117671251297, 0.690150260925293, 0.6889018416404724, 0.6883252263069153], 'binary_accuracy': [0.5257142782211304, 0.5257142782211304, 0.5257142782211304, 0.5257142782211304, 0.5257142782211304], 'val_loss': [0.6914997100830078, 0.6912367939949036, 0.6909546256065369, 0.6906673908233643, 0.6903428435325623], 'val_binary_accuracy': [0.5266666412353516, 0.5266666412353516, 0.5266666412353516, 0.5266666412353516, 0.5266666412353516]}\n",
      "0.5266666412353516\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f1f930400c4cd697af166a42f8df03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">summer-sweep-5</strong>: <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/ccsal6db\" target=\"_blank\">https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/ccsal6db</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220803_165743-ccsal6db/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hc8fr947 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcallbacks_log_dir: logs/run/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop1: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop2: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_min_delta: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tglobal_ds_size: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_split: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_split: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvocab_size: 15000\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/erwan/Programmes/Stage/dlexperiments/Erwan/Tweeter_Dataset/wandb/run-20220803_165814-hc8fr947</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/hc8fr947\" target=\"_blank\">fresh-sweep-6</a></strong> to <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/sweeps/xn3q70sq\" target=\"_blank\">https://wandb.ai/erwanlbv/Remote-PureEmb-tf/sweeps/xn3q70sq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      " Ensemble de données créé, taille : 1000\n",
      "------------\n",
      " Taille des ensembles de données : 700, 150\n",
      "Fin du chargement des bases de données\n",
      "700 150 150\n",
      "------------\n",
      "Fin de l'entraînement du tokenizer\n",
      "------------\n",
      "Fin de la préparation des bases de donneés\n",
      "1000\n",
      "-----------\n",
      "Entraînement lancé \n",
      " - Nom : fresh-sweep-6 \n",
      " - Configuration : {'batch_size': 50, 'callbacks_log_dir': 'logs/run/', 'drop1': 0.4, 'drop2': 0.2, 'early_stopping_min_delta': 0.001, 'embedding_dim': 500, 'epochs': 5, 'global_ds_size': 1000, 'lr': 0.0008, 'max_length': 200, 'train_split': 0.7, 'val_split': 0.15, 'vocab_size': 15000, 'architecture_name': 'PureEmbedding', 'dataset_name': 'tweeter-fr'} \n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6945 - binary_accuracy: 0.5057\n",
      "Epoch 1: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 3s 165ms/step - loss: 0.6945 - binary_accuracy: 0.5057 - val_loss: 0.6948 - val_binary_accuracy: 0.4400\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6924 - binary_accuracy: 0.5057\n",
      "Epoch 2: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 0.6924 - binary_accuracy: 0.5057 - val_loss: 0.6926 - val_binary_accuracy: 0.4400\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6917 - binary_accuracy: 0.5057\n",
      "Epoch 3: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 2s 145ms/step - loss: 0.6917 - binary_accuracy: 0.5057 - val_loss: 0.6923 - val_binary_accuracy: 0.4400\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6910 - binary_accuracy: 0.5057\n",
      "Epoch 4: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 0.6910 - binary_accuracy: 0.5057 - val_loss: 0.6923 - val_binary_accuracy: 0.4400\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6901 - binary_accuracy: 0.5057\n",
      "Epoch 5: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 2s 136ms/step - loss: 0.6901 - binary_accuracy: 0.5057 - val_loss: 0.6921 - val_binary_accuracy: 0.4400\n",
      "{'loss': [0.6945209503173828, 0.6924110054969788, 0.6916753649711609, 0.690996527671814, 0.690070390701294], 'binary_accuracy': [0.5057142972946167, 0.5057142972946167, 0.5057142972946167, 0.5057142972946167, 0.5057142972946167], 'val_loss': [0.6947909593582153, 0.692578911781311, 0.6922549605369568, 0.6922614574432373, 0.6920782327651978], 'val_binary_accuracy': [0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142]}\n",
      "0.4399999976158142\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbc372691ef42928c6dbbaaf94b8ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fresh-sweep-6</strong>: <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/hc8fr947\" target=\"_blank\">https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/hc8fr947</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220803_165814-hc8fr947/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yz8u7nfm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcallbacks_log_dir: logs/run/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop1: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop2: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_min_delta: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tglobal_ds_size: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_split: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_split: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvocab_size: 20000\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/erwan/Programmes/Stage/dlexperiments/Erwan/Tweeter_Dataset/wandb/run-20220803_165841-yz8u7nfm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/yz8u7nfm\" target=\"_blank\">peachy-sweep-7</a></strong> to <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/sweeps/xn3q70sq\" target=\"_blank\">https://wandb.ai/erwanlbv/Remote-PureEmb-tf/sweeps/xn3q70sq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      " Ensemble de données créé, taille : 1000\n",
      "------------\n",
      " Taille des ensembles de données : 700, 150\n",
      "Fin du chargement des bases de données\n",
      "700 150 150\n",
      "------------\n",
      "Fin de l'entraînement du tokenizer\n",
      "------------\n",
      "Fin de la préparation des bases de donneés\n",
      "1000\n",
      "-----------\n",
      "Entraînement lancé \n",
      " - Nom : peachy-sweep-7 \n",
      " - Configuration : {'batch_size': 50, 'callbacks_log_dir': 'logs/run/', 'drop1': 0.2, 'drop2': 0.2, 'early_stopping_min_delta': 0.001, 'embedding_dim': 200, 'epochs': 5, 'global_ds_size': 1000, 'lr': 0.0008, 'max_length': 200, 'train_split': 0.7, 'val_split': 0.15, 'vocab_size': 20000, 'architecture_name': 'PureEmbedding', 'dataset_name': 'tweeter-fr'} \n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.6939 - binary_accuracy: 0.4700\n",
      "Epoch 1: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 2s 73ms/step - loss: 0.6936 - binary_accuracy: 0.4729 - val_loss: 0.6927 - val_binary_accuracy: 0.4867\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6915 - binary_accuracy: 0.4729\n",
      "Epoch 2: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 1s 68ms/step - loss: 0.6915 - binary_accuracy: 0.4729 - val_loss: 0.6929 - val_binary_accuracy: 0.4867\n",
      "Epoch 3/5\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.6902 - binary_accuracy: 0.4700\n",
      "Epoch 3: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 1s 64ms/step - loss: 0.6905 - binary_accuracy: 0.4729 - val_loss: 0.6929 - val_binary_accuracy: 0.4867\n",
      "Epoch 4/5\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6896 - binary_accuracy: 0.4769\n",
      "Epoch 4: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 1s 68ms/step - loss: 0.6891 - binary_accuracy: 0.4729 - val_loss: 0.6928 - val_binary_accuracy: 0.4867\n",
      "Epoch 5/5\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6892 - binary_accuracy: 0.4769\n",
      "Epoch 5: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 0.6889 - binary_accuracy: 0.4729 - val_loss: 0.6926 - val_binary_accuracy: 0.4867\n",
      "{'loss': [0.6936363577842712, 0.6915478706359863, 0.6905078291893005, 0.6891317963600159, 0.6888540387153625], 'binary_accuracy': [0.47285714745521545, 0.47285714745521545, 0.47285714745521545, 0.47285714745521545, 0.47285714745521545], 'val_loss': [0.6926555633544922, 0.692887008190155, 0.6929312348365784, 0.692787766456604, 0.6926128268241882], 'val_binary_accuracy': [0.4866666793823242, 0.4866666793823242, 0.4866666793823242, 0.4866666793823242, 0.4866666793823242]}\n",
      "0.4866666793823242\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db464444cd4477d89226e75047f0f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">peachy-sweep-7</strong>: <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/yz8u7nfm\" target=\"_blank\">https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/yz8u7nfm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220803_165841-yz8u7nfm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p8s7641q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcallbacks_log_dir: logs/run/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop1: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdrop2: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_min_delta: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tglobal_ds_size: 1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_split: 0.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_split: 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvocab_size: 30000\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/erwan/Programmes/Stage/dlexperiments/Erwan/Tweeter_Dataset/wandb/run-20220803_165902-p8s7641q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/p8s7641q\" target=\"_blank\">leafy-sweep-8</a></strong> to <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/sweeps/xn3q70sq\" target=\"_blank\">https://wandb.ai/erwanlbv/Remote-PureEmb-tf/sweeps/xn3q70sq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      " Ensemble de données créé, taille : 1000\n",
      "------------\n",
      " Taille des ensembles de données : 700, 150\n",
      "Fin du chargement des bases de données\n",
      "700 150 150\n",
      "------------\n",
      "Fin de l'entraînement du tokenizer\n",
      "------------\n",
      "Fin de la préparation des bases de donneés\n",
      "1000\n",
      "-----------\n",
      "Entraînement lancé \n",
      " - Nom : leafy-sweep-8 \n",
      " - Configuration : {'batch_size': 50, 'callbacks_log_dir': 'logs/run/', 'drop1': 0.4, 'drop2': 0.3, 'early_stopping_min_delta': 0.001, 'embedding_dim': 100, 'epochs': 5, 'global_ds_size': 1000, 'lr': 0.0008, 'max_length': 200, 'train_split': 0.7, 'val_split': 0.15, 'vocab_size': 30000, 'architecture_name': 'PureEmbedding', 'dataset_name': 'tweeter-fr'} \n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.6933 - binary_accuracy: 0.5354\n",
      "Epoch 1: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 1s 61ms/step - loss: 0.6937 - binary_accuracy: 0.5286 - val_loss: 0.6889 - val_binary_accuracy: 0.5867\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6916 - binary_accuracy: 0.5286\n",
      "Epoch 2: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 1s 55ms/step - loss: 0.6916 - binary_accuracy: 0.5286 - val_loss: 0.6860 - val_binary_accuracy: 0.5867\n",
      "Epoch 3/5\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.6900 - binary_accuracy: 0.5364\n",
      "Epoch 3: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.6910 - binary_accuracy: 0.5286 - val_loss: 0.6847 - val_binary_accuracy: 0.5867\n",
      "Epoch 4/5\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.6897 - binary_accuracy: 0.5317\n",
      "Epoch 4: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 1s 57ms/step - loss: 0.6901 - binary_accuracy: 0.5286 - val_loss: 0.6842 - val_binary_accuracy: 0.5867\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6901 - binary_accuracy: 0.5286\n",
      "Epoch 5: val_binary_accuracy did not improve from 0.60000\n",
      "14/14 [==============================] - 1s 47ms/step - loss: 0.6901 - binary_accuracy: 0.5286 - val_loss: 0.6840 - val_binary_accuracy: 0.5867\n",
      "{'loss': [0.6937175393104553, 0.6915575265884399, 0.6910386681556702, 0.6901417970657349, 0.690094530582428], 'binary_accuracy': [0.5285714268684387, 0.5285714268684387, 0.5285714268684387, 0.5285714268684387, 0.5285714268684387], 'val_loss': [0.6889154314994812, 0.686004638671875, 0.6847012639045715, 0.6841636300086975, 0.6840325593948364], 'val_binary_accuracy': [0.5866666436195374, 0.5866666436195374, 0.5866666436195374, 0.5866666436195374, 0.5866666436195374]}\n",
      "0.5866666436195374\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc90da277e84288bcdfb0b6d4d82712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">leafy-sweep-8</strong>: <a href=\"https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/p8s7641q\" target=\"_blank\">https://wandb.ai/erwanlbv/Remote-PureEmb-tf/runs/p8s7641q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220803_165902-p8s7641q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=sweep_train, count=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tensorF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "109a7cdcd8654e0ad7600619333c61c6996e73d2e81b79f582612eb56b6147e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
