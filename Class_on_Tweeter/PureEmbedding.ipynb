{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## À FAIRE : \n",
    "- S'occuper des accents (demander à Houssam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération de l'ensemble de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/erwan/Programmes/Stage/dlexperiments/Erwan/Text_Classification/datasets/Tweeter/french_tweets.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1199141</th>\n",
       "      <td>1</td>\n",
       "      <td>Productif mais pas aussi calme que id'a a aimé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082000</th>\n",
       "      <td>1</td>\n",
       "      <td>Je n'ai pas ... non, je t'ai eu la chose en ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357253</th>\n",
       "      <td>1</td>\n",
       "      <td>Nous utilisons chipotle tabasco pour les ailes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693626</th>\n",
       "      <td>0</td>\n",
       "      <td>Je veux regarder me traîner vers l'enfer jaloux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368336</th>\n",
       "      <td>1</td>\n",
       "      <td>Rien ne réchauffe mon éloge que de s'asseoir d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141882</th>\n",
       "      <td>1</td>\n",
       "      <td>Je suis juste revenu de travailler. J'ai vu un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746381</th>\n",
       "      <td>0</td>\n",
       "      <td>Idk vraiment. Le safari s'arrête également bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433638</th>\n",
       "      <td>1</td>\n",
       "      <td>Indépendamment de ce qui se passe à partir de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460310</th>\n",
       "      <td>0</td>\n",
       "      <td>Appareil photo, soyez d'accord.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427179</th>\n",
       "      <td>1</td>\n",
       "      <td>Aaaaaaaaawawwwwwwwwwwwwww high school sweethea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text\n",
       "1199141      1  Productif mais pas aussi calme que id'a a aimé...\n",
       "1082000      1  Je n'ai pas ... non, je t'ai eu la chose en ti...\n",
       "1357253      1  Nous utilisons chipotle tabasco pour les ailes...\n",
       "693626       0    Je veux regarder me traîner vers l'enfer jaloux\n",
       "1368336      1  Rien ne réchauffe mon éloge que de s'asseoir d...\n",
       "...        ...                                                ...\n",
       "1141882      1  Je suis juste revenu de travailler. J'ai vu un...\n",
       "746381       0  Idk vraiment. Le safari s'arrête également bea...\n",
       "1433638      1  Indépendamment de ce qui se passe à partir de ...\n",
       "460310       0                    Appareil photo, soyez d'accord.\n",
       "1427179      1  Aaaaaaaaawawwwwwwwwwwwwww high school sweethea...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_size = 1000\n",
    "df = df[:df_size]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1199141    1\n",
       "1082000    1\n",
       "1357253    1\n",
       "693626     0\n",
       "1368336    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = df['label']\n",
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1199141    Productif mais pas aussi calme que id'a a aimé...\n",
       "1082000    Je n'ai pas ... non, je t'ai eu la chose en ti...\n",
       "1357253    Nous utilisons chipotle tabasco pour les ailes...\n",
       "693626       Je veux regarder me traîner vers l'enfer jaloux\n",
       "1368336    Rien ne réchauffe mon éloge que de s'asseoir d...\n",
       "                                 ...                        \n",
       "1141882    Je suis juste revenu de travailler. J'ai vu un...\n",
       "746381     Idk vraiment. Le safari s'arrête également bea...\n",
       "1433638    Indépendamment de ce qui se passe à partir de ...\n",
       "460310                       Appareil photo, soyez d'accord.\n",
       "1427179    Aaaaaaaaawawwwwwwwwwwwwww high school sweethea...\n",
       "Name: text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences = df['text']\n",
    "text_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-03 15:39:12.708118: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((text_sequences, targets))\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ds = ds.batch(32)\n",
    "iterator_ds = iter(b_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32]), TensorShape([32]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, label = next(iterator_ds)\n",
    "text.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets(df, ds_size, batch_size, vocab_size, max_length):\n",
    "    print(\"------------\")\n",
    "    \n",
    "    def create_ds(df, size):\n",
    "        shuffled_df = shuffle(df)[:size]\n",
    "        text_seq = shuffled_df['text']\n",
    "        target_seq = shuffled_df['label']\n",
    "        ds = tf.data.Dataset.from_tensor_slices((text_seq, target_seq))\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "    ds = create_ds(df, ds_size)\n",
    "    ds_size = len(ds)    \n",
    "    print(f\" Ensemble de données créé, taille : {ds_size}\")   \n",
    "    print(\"------------\")\n",
    "\n",
    "\n",
    "    train_size = int(0.7 * ds_size)\n",
    "    val_size = int(0.15 * ds_size)\n",
    "    print(f\" Taille des ensembles de données : {train_size}, {val_size}\")\n",
    "    \n",
    "    ds.shuffle(1)\n",
    "\n",
    "    str_train_ds = ds.take(train_size).batch(batch_size)\n",
    "    str_val_ds = ds.skip(train_size).take(val_size).batch(batch_size)\n",
    "    str_test_ds = ds.skip(train_size + val_size).batch(batch_size)\n",
    "\n",
    "    print(\"Fin du chargement des bases de données\")\n",
    "    print(len(str_train_ds) * batch_size, len(str_val_ds) * batch_size, len(str_test_ds) * batch_size)\n",
    "    print(\"------------\")\n",
    "\n",
    "    tokenizer_layer = tf.keras.layers.TextVectorization(\n",
    "        standardize='lower_and_strip_punctuation',\n",
    "        split='whitespace',\n",
    "        max_tokens=vocab_size,\n",
    "        output_sequence_length=max_length,\n",
    "    )\n",
    "\n",
    "    # On entraine le tokenizer sur l'ensemble de données d'entraînement\n",
    "    tokenizer_layer.adapt(str_train_ds.map(lambda text, label: text))\n",
    "    print(\"Fin de l'entraînement du tokenizer\")\n",
    "    print(\"------------\")\n",
    "\n",
    "\n",
    "    # Préparation des ensembles de données : \n",
    "    def tokenize_text(text, label):\n",
    "        text = tf.expand_dims(text, -1) # Explication -1 -> tf.data.Dataset -> \"map\"\n",
    "        res = tokenizer_layer(text)\n",
    "        \n",
    "        return res, label\n",
    "\n",
    "    train_ds = str_train_ds.map(tokenize_text)\n",
    "    val_ds = str_val_ds.map(tokenize_text)\n",
    "    test_ds = str_test_ds.map(tokenize_text)\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    print(\"Fin de la préparation des bases de donneés\")\n",
    "    print(len(train_ds) * batch_size, len(val_ds) * batch_size, len(test_ds) * batch_size)\n",
    "    print(\"-----------\")\n",
    "\n",
    "    return tokenizer_layer, train_ds, val_ds, test_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformation dataframe -> ensemble de données tf (avec batch) semble avoir fonctionnée. On remarque cependant un petit problème au niveau de __l'encodage utf-8__ (dû à la présence d'accents dans la langue française) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 150 # Dimension avec laquelle on représente nos jetons\n",
    "\n",
    "def build_network(vocab_size, embedding_dim, summary):\n",
    "\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1),  # activation=None de base, la sortie n'est donc pas normalisée\n",
    "  ])\n",
    "\n",
    "  model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    "  )\n",
    "\n",
    "  if summary:\n",
    "      model.summary()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/erwan/Programmes/Stage/dlexperiments/Erwan/Text_Classification/datasets/Tweeter/french_tweets.csv\"\n",
    "\n",
    "df = pd.read_csv(PATH)\n",
    "i = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_SIZE = 10000 # Ensemble de données entier : 1526724\n",
    "BATCH_SIZE = 200\n",
    "VOCAB_SIZE = 5000\n",
    "EMBEDDING_DIM = 200\n",
    "MAX_LENGTH = EMBEDDING_DIM\n",
    "\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i += 1\n",
    "num_run = \"0\" + str(i)\n",
    "\n",
    "log_dir = \"logs/run/\" + num_run\n",
    "log_model = 'logs/run/' + num_run + '/models'\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, \n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "\n",
    "model_checkpoint_callbacks = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=log_model,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_binary_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    initial_value_threshold=0.60,\n",
    "    verbose=1\n",
    ")s\n",
    "\n",
    "early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_binary_accuracy',\n",
    "    min_delta=0.001,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      " Ensemble de données créé, taille : 10000\n",
      "------------\n",
      " Taille des ensembles de données : 7000, 1500\n",
      "Fin du chargement des bases de données\n",
      "7000 1600 1600\n",
      "------------\n",
      "Fin de l'entraînement du tokenizer\n",
      "------------\n",
      "Fin de la préparation des bases de donneés\n",
      "7000 1600 1600\n",
      "-----------\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 200)         1000000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 200)         0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 200)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,000,201\n",
      "Trainable params: 1,000,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tokenizer, train_ds, val_ds, test_ds = build_datasets(\n",
    "    df=df,\n",
    "    ds_size=DS_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "model = build_network(VOCAB_SIZE, EMBEDDING_DIM, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement pour un ensemble de données de 10000 exemples dont 7000 pour l'entraînement\n",
      "\n",
      "Epoch 1/5\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6929 - binary_accuracy: 0.5079\n",
      "Epoch 1: val_binary_accuracy did not improve from 0.60000\n",
      "35/35 [==============================] - 3s 71ms/step - loss: 0.6929 - binary_accuracy: 0.5079 - val_loss: 0.6930 - val_binary_accuracy: 0.4920\n",
      "Epoch 2/5\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6926 - binary_accuracy: 0.5283\n",
      "Epoch 2: val_binary_accuracy did not improve from 0.60000\n",
      "35/35 [==============================] - 2s 52ms/step - loss: 0.6926 - binary_accuracy: 0.5283 - val_loss: 0.6927 - val_binary_accuracy: 0.4920\n",
      "Epoch 3/5\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.6922 - binary_accuracy: 0.5381\n",
      "Epoch 3: val_binary_accuracy did not improve from 0.60000\n",
      "35/35 [==============================] - 2s 54ms/step - loss: 0.6922 - binary_accuracy: 0.5384 - val_loss: 0.6924 - val_binary_accuracy: 0.4940\n",
      "Epoch 4/5\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.6919 - binary_accuracy: 0.5485\n",
      "Epoch 4: val_binary_accuracy did not improve from 0.60000\n",
      "35/35 [==============================] - 2s 53ms/step - loss: 0.6918 - binary_accuracy: 0.5500 - val_loss: 0.6920 - val_binary_accuracy: 0.5093\n",
      "Epoch 5/5\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.6914 - binary_accuracy: 0.5613\n",
      "Epoch 5: val_binary_accuracy did not improve from 0.60000\n",
      "35/35 [==============================] - 2s 51ms/step - loss: 0.6914 - binary_accuracy: 0.5613 - val_loss: 0.6916 - val_binary_accuracy: 0.5640\n",
      "{'loss': [0.692918598651886, 0.6925840377807617, 0.6921752095222473, 0.6918370723724365, 0.6914048790931702], 'binary_accuracy': [0.5078571438789368, 0.5282857418060303, 0.538428544998169, 0.550000011920929, 0.5612857341766357], 'val_loss': [0.6930367946624756, 0.6927042603492737, 0.6923871040344238, 0.692025363445282, 0.6916399598121643], 'val_binary_accuracy': [0.492000013589859, 0.492000013589859, 0.49399998784065247, 0.5093333125114441, 0.5640000104904175]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entraînement pour un ensemble de données de {DS_SIZE} exemples dont {len(train_ds) * BATCH_SIZE} pour l'entraînement\")\n",
    "print()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tensorboard_callback, model_checkpoint_callbacks, early_stopping_callbacks],\n",
    ")\n",
    "\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 1ms/step - loss: 0.7834 - binary_accuracy: 0.6924\n",
      "Erreur : 0.783367395401001\n",
      "Précision: 0.6923999786376953\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(f\"Erreur : {loss}\")\n",
    "print(f\"Précision : {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "    tokenizer,\n",
    "    model\n",
    "]) #Ajouter une sigmoïde ?\n",
    "\n",
    "export_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n",
    "    optimizer=\"adam\",\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model.save('logs/export_models/model' + str(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tensorF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "109a7cdcd8654e0ad7600619333c61c6996e73d2e81b79f582612eb56b6147e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
