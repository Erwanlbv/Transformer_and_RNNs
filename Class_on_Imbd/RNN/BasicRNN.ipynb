{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier du texte avec un RNN\n",
    "\n",
    "* Base : https://www.tensorflow.org/text/tutorials/text_classification_rnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de l'ensemble de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 12:43:53.003074: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seed = 1\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '/Users/erwan/Programmes/Stage/dlexperiments/Erwan/Text_Classification/aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '/Users/erwan/Programmes/Stage/dlexperiments/Erwan/Text_Classification/aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    '/Users/erwan/Programmes/Stage/dlexperiments/Erwan/Text_Classification/aclImdb/test',\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation des ensembles de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-traitement de l'ensemble de données \n",
    "vocab_size = 1000\n",
    "output_sequence_length = 150\n",
    "\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=output_sequence_length,\n",
    ")\n",
    "\n",
    "tokenizer.adapt(raw_train_ds.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode .adapt définit le vocabulaire de la couche. On les appelle \"jetons\". Ceux placés en premiers sont : l'espace (' ') et les masques inconnus, ils sont ensuite triés par fréquence.\n",
    "\n",
    "Voici les 15 premiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocabulary() # list\n",
    "vocab[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de segmentation de l'ensemble des données\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    vectorized_text = tokenizer(text)\n",
    "    return vectorized_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On segmente les ensembles de données\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Et on les prépare à l'entraînement\n",
    "### Sans Autotune c'est environ 2 sec de plus d'entraînement sur la \n",
    "### première epoch et uen sec de plus sur toutes les autres, même\n",
    "### sur une base de données aussi petite et un modèle aussi simple.\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec une dimension faible de l'espace d'encodage, le processus n'est pas complètement réversible. Deux raisons principales à cela :\n",
    "\n",
    "- La valeur par défaut pour l'arg de TextVectorization(standardize=arg)  est \"lower_and_strip_punctuation\", c'est à dire que la ponctuatio est retirée et les majuscules replacées par des minuscules.\n",
    "- La taille limitée du vocabulaire et l'absence de solution de secours basée sur les caractères donnent lieu à des jetons inconnus. (Voir page *Layers/TextVectorization*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "\n",
    "emb_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=len(vocab),\n",
    "    output_dim=embedding_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 20)          20000     \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 60)                4860      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 40)                2440      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,341\n",
      "Trainable params: 27,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    emb_layer,\n",
    "    tf.keras.layers.SimpleRNN(60),\n",
    "    tf.keras.layers.Dense(40, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La couche de TextVect correspond à l'encoder  \n",
    "La couche d'embedding est ajoutée pour sa capacité à indexer les textes vectorisés d'une manière qui améliore grandement l'apprentissage. Elle bien plus efficace que l'opération consistant à faire passer l'équivalent d'un vecteur codé à chaud à travers une couche Dense.  \n",
    "La couche de RNN est celle du LSTM  \n",
    "On termine avec des Dense pour la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(5e-4),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 41s 61ms/step - loss: 0.5332 - binary_accuracy: 0.7149 - val_loss: 0.4765 - val_binary_accuracy: 0.7932\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 40s 64ms/step - loss: 0.4044 - binary_accuracy: 0.8231 - val_loss: 0.4397 - val_binary_accuracy: 0.8096\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 42s 67ms/step - loss: 0.3898 - binary_accuracy: 0.8273 - val_loss: 0.4330 - val_binary_accuracy: 0.8130\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 41s 65ms/step - loss: 0.3818 - binary_accuracy: 0.8326 - val_loss: 0.4242 - val_binary_accuracy: 0.8156\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 42s 67ms/step - loss: 0.3814 - binary_accuracy: 0.8328 - val_loss: 0.4168 - val_binary_accuracy: 0.8148\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remarque :  \n",
    "Il est bien plus rapide de vectoriser les ensembles de données avec avec TextVect puis d'entraîner un modèle sans TextVect sur les bases de données obtenus que d'inclure la couche TextVect dans le Sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 15s 20ms/step - loss: 0.4093 - binary_accuracy: 0.8208\n",
      "Perte : 0.40926551818847656, Précision : 0.8208000063896179\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(f\"Perte : {loss}, Précision : {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un peu de dessous de la simple couche d'embedding avec du GlobalPolling.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportation du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "    tokenizer,\n",
    "    model,\n",
    "    tf.keras.layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 21s 25ms/step - loss: 0.4093 - accuracy: 0.8208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40926527976989746, 0.8208000063896179]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_model.evaluate(raw_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les performances sont identiques : ok !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédicitions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 74ms/step\n",
      "[[0.41065142]]\n"
     ]
    }
   ],
   "source": [
    "sample_text = ('The movie was not good, it was incredibly good.')\n",
    "\n",
    "predictions = export_model.predict([sample_text])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('IA-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "701f8f61e2df02aa7711dfd09ce0ea30c74b88eb04e87f301847efa4bff5d6ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
