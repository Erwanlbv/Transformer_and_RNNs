{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificateur de texte basique (intro to Tf_hub)\n",
    "\n",
    "Avec la base de données IMBD (50 000 avis sur des films pos/neg)\n",
    "* Bases : https://www.tensorflow.org/tutorials/keras/text_classification * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les __aclImdb/train/pos__ et __aclImdb/train/neg__ contiennent de nombreux fichiers texte, dont chacun est une critique de film unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = os.path.join(train_dir, 'pos/1181_9.txt')\n",
    "with open(sample) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du jeu de données dans un format adapté à l'entrainement pour tf :  \n",
    "main_directory/  \n",
    "...class_a/   \n",
    "......a_text_1.txt  \n",
    "......a_text_2.txt  \n",
    "...class_b/  \n",
    "......b_text_1.txt  \n",
    "......b_text_2.txt  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "print(os.listdir(test_dir))\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "print(os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On divise la base de données en 3 parties :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print(\"Review\", text_batch.numpy()[i])\n",
    "        print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds.class_names[0], raw_train_ds.class_names[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble de validation :\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=seed)\n",
    "\n",
    "# Remarquer le changement 'validation' en subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble de test :\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', \n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Préparation de l'ensemble de données :\n",
    "\n",
    "Standardiser, tokeniser et vectoriser les données à l'aide de la couche __tf.keras.layers.TextVectorization__\n",
    "\n",
    "- La __standardisation__ fait référence au prétraitement du texte, généralement pour supprimer la ponctuation ou les éléments HTML afin de simplifier l'ensemble de données.  \n",
    "- La __tokenisation__ fait référence à la division de chaînes en jetons (par exemple, la division d'une phrase en mots individuels, en la divisant sur des espaces)\n",
    "- La __vectorisation__ fait référence à la conversion de jetons en nombres (vecteurs) afin qu'ils puissent être introduits dans un réseau de neurones. \n",
    "\n",
    "__TextVectorization__ est une couche qui permet d'accomplir toutes ces tâches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comme vous l'avez vu ci-dessus, les avis contiennent diverses balises HTML comme \\<br /> . __Ces balises ne seront pas supprimées par le standardiseur par défaut dans la couche TextVectorization__ (qui convertit le texte en minuscules et supprime la ponctuation par défaut, mais ne supprime pas le HTML). Vous écrirez une fonction de standardisation personnalisée pour supprimer le code HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    \n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "    '[%s]' % re.escape(string.punctuation),\n",
    "    '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =['This is, punctuation, A not well positionned mAJ..',\n",
    "'Other data',\n",
    "'And here we go',\n",
    "'Phrase ']\n",
    "\n",
    "default_tokenizer = tf.keras.layers.TextVectorization()\n",
    "default_tokenizer.adapt(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_text = default_tokenizer(data)\n",
    "tok_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On créé un ensemble de données sans label \n",
    "train_text = raw_train_ds.map(lambda text, label: text)\n",
    "\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "tokenizer_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "# On créé le vocabulaire \n",
    "tokenizer_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = next(iter(train_text)) # On sort un batch de texte\n",
    "first_text = text[0]          # On prend le premier text du batch  \n",
    "\n",
    "len(text), first_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  tf.expand_dims(first_text, -1)\n",
    "text, tokenizer_layer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    vectorized_text = vectorize_layer(text)\n",
    "    return vectorized_text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__tf.expand_dims(intput, axis)__ : Returns a tensor with a length 1 axis inserted at index axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "\n",
    "print(\"Review: \", first_review)\n",
    "print(\"Label: \", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized review: \",vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vectorise chacun de nos avis sur un vecteur de taille 250, si l'avis est assez bref il y aura beaucoup de 0 dans ce vecteur, et inversement.\n",
    "C'est à partir de cette forme de codage que l'analyse de \"sentiment\" sera effectuée.  \n",
    "\n",
    "La grandeur 'max_features' correspond au nombre maximal de \"mots\" considérés dans notre \"vectorize_layer\". (Un terme plus correcte serait *ensemble de caractères*)\n",
    "\n",
    "\n",
    "Il est possible de faire l'opération inverse (entier -> jeton) avec la commande __.get_vocabulary()['int']__ \n",
    "\n",
    "---- __Note :__ puisqu'il s'agit d'une classification en 2 classes, serait-il possible d'utiliser des SVM ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
    "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisons les performances d'entrainement de l'algo (voir le notebook dédié) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du réseau de neurones :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Embedding(max_features + 1, embedding_dim),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1),\n",
    "  tf.keras.layers.Activation('sigmoid')]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer='adam',\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(f\"Erreur : {loss}\")\n",
    "print(f\"Précision: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['This was a incredibly deceptive movie',\n",
    "'I was expecting it to be bad, I was good',\n",
    "'Nothing to deal with sentiment and movie here',\n",
    "'That was incredible']\n",
    "\n",
    "vect_data = vectorize_layer(data)\n",
    "model.predict(vect_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporter le modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  tf.keras.layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('IA-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "701f8f61e2df02aa7711dfd09ce0ea30c74b88eb04e87f301847efa4bff5d6ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
