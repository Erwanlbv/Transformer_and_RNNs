{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation du modèle de traduction de *Cho et al* __(2014)__\n",
    "https://arxiv.org/pdf/1406.1078.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "\n",
    "from torchtext import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Tokenizer\n",
    "On va utiliser un tokenizer BPE avec une séparation à minima sur les espaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "from tokenizers.normalizers import NFD\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option vocab\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.normalizer = NFD()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[CLS]\", \"[PAD]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer =  BpeTrainer(\n",
    "    vocab=10000,\n",
    "    special_tokens=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(data, tokenizer, trainer, save=False):\n",
    "\n",
    "    files = ''\n",
    "    tokenizer.train(files, trainer)\n",
    "    \n",
    "    # Saving\n",
    "    if save:\n",
    "        tokenizer.save('tokenizers/001.json')\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "data = ['blabla']\n",
    "\n",
    "tokenizer.train_from_iterator(data, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'bla']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode('This is bla')\n",
    "output.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodeur RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, hidden_size, num_layers=1, vocab_size=1000) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size, \n",
    "            embedding_dim=self.emb_dim,\n",
    "            )\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.emb_dim,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            nonlinearity='tanh', # tanh par défaut, 'relu' possible\n",
    "            batch_first=True, # Sortie (batch_size, sequence_length, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_sequence, hidden):\n",
    "        # Input,  :   [batch_size, sequence_length], \n",
    "        # Hidden  :   [batch_size, sequence_length, hidden_size]\n",
    "        \n",
    "        # Embedding :       [batch_size, sequence_length, embedding_dim] (emb_dim = hidden_size ici)\n",
    "        # Output :          [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "        embedded = self.embedding(input_sequence)            # Entrée\n",
    "        output_sequence, hidden = self.rnn(embedded, hidden) # État précédent\n",
    "\n",
    "        return output_sequence, hidden\n",
    "\n",
    "    def h_init(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • Vérifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeur = Encoder(\n",
    "    emb_dim=100, \n",
    "    hidden_size=50,\n",
    "    num_layers=2)\n",
    "\n",
    "# batch_size, sequence_length\n",
    "emb_input = torch.randint(1, size=(32, 10))   \n",
    "\n",
    "# num_layer, sequence_length, hidden_size\n",
    "hidden_input = torch.randint(1, size=(2, 32, 50), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 10, 50]), torch.Size([2, 32, 50]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = encodeur(emb_input, hidden_input) # output, hidden\n",
    "res[0].shape, res[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Décodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decodeur(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dim, hidden_size, num_layers=1, vocab_size=1000) -> None:\n",
    "        super(Decodeur, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = emb_dim        # Dimension des y_t\n",
    "        self.vocab_size = vocab_size        # Dimension des prédictions\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim=self.embedding_dim\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = self.embedding_dim + self.hidden_size, # Concaténation entre \"input\" et context\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            nonlinearity='tanh',\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(\n",
    "            in_features = 2 * self.hidden_size + self.embedding_dim,\n",
    "            out_features=self.vocab_size, # Dimension des pred\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_sequence, context):\n",
    "        \n",
    "        # pred : batch_size, sequence_length, decoder_vocab_size\n",
    "        # input : batch_size, sequence_length\n",
    "    \n",
    "        seq_length = input_sequence.shape[1]\n",
    "        emb_input_seq = self.embedding(input_sequence) # batch_size, sequence_length, emb_dim\n",
    "\n",
    "        # Voir cours RNN papier pour explications sur la reproduction du contexte\n",
    "        c = context[-1]\n",
    "        c = torch.unsqueeze(c, dim=1)\n",
    "        c = torch.cat((c, ) * seq_length, dim=1)\n",
    "\n",
    "        z_concat = torch.cat((emb_input_seq, c), dim=2) \n",
    "\n",
    "        output_seq, hidden =  self.rnn(z_concat, context)\n",
    "        output_concat = torch.cat((z_concat, output_seq), dim=2)\n",
    "\n",
    "        y = self.linear(output_concat)\n",
    "        sequence_preds = self.softmax(y)\n",
    "\n",
    "        print(f\"emb : {emb_input_seq.shape}\")\n",
    "        print(f\"c : {c.shape}\")\n",
    "        print(f\"z_concat : {z_concat.shape}\")\n",
    "        print(f\"hidden : {hidden.shape}\")\n",
    "        print(f\"output_concat : {output_concat.shape}\")\n",
    "        print(f\"sequence_preds : {sequence_preds.shape}\")\n",
    "\n",
    "        return sequence_preds, hidden\n",
    "\n",
    "    def h_init(self, batch_size):\n",
    "        # num_layers, batch_size=1, output_dim\n",
    "        y_init =  torch.zeros(size=(self.num_layers, batch_size, self.embedding_dim))\n",
    "        return y_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb : torch.Size([32, 15, 150])\n",
      "c : torch.Size([32, 15, 60])\n",
      "z_concat : torch.Size([32, 15, 210])\n",
      "hidden : torch.Size([2, 32, 60])\n",
      "output_concat : torch.Size([32, 15, 270])\n",
      "sequence_preds : torch.Size([32, 15, 1000])\n"
     ]
    }
   ],
   "source": [
    "decodeur = Decodeur(\n",
    "    emb_dim=150, \n",
    "    hidden_size=60,\n",
    "    num_layers=2, \n",
    "    vocab_size=1000)\n",
    "\n",
    "# batch_size,  (Prédiction à l'instant t-1)\n",
    "dec_input_sequence = torch.randint(1000, size=(32, 15)) \n",
    "\n",
    "#  num_layers, batch_size, hidden_size\n",
    "context = torch.randint(1, size=(2, 32, 60), dtype=torch.float) # Fait office de h_init\n",
    "\n",
    "\n",
    "dec_res = decodeur(dec_input_sequence, context) # context fait office de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemblage Encodeur - Décodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder) -> None:\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source_seq, target_seq):\n",
    "        \n",
    "        # source_sequence : batch_size, seq_len, encodeur_vocab_size\n",
    "        # target_sequence : batch_size, seq_len, decodeur_vocab_size\n",
    "        batch_size = source_seq.shape[0]\n",
    "        h_init = self.encoder.h_init(batch_size)\n",
    "\n",
    "        _, context = self.encoder(source_seq, h_init) # Seul le contexte nous intéresse\n",
    "        pred_seq, _ = self.decoder(target_seq, context) # Seul les résultats de la dernière couche nous intéressent\n",
    "\n",
    "        preds = torch.argmax(pred_seq, dim=2) # On cherche le max sur la dernière dim\n",
    "        print(f\"preds : {preds.shape}\")\n",
    "\n",
    "        return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"encoder_embedding_dim\": 150,\n",
    "    \"encoder_vocab_size\": 15000,\n",
    "    \"encoder_num_layers\": 2,\n",
    "    \"decoder_embedding_dim\": 100,\n",
    "    \"decoder_vocab_size\": 10000,\n",
    "    \"decoder_num_layers\": 2,\n",
    "    \"hidden_size\": 50,\n",
    "}\n",
    "\n",
    "encodeur = Encoder(\n",
    "    emb_dim=config[\"encoder_embedding_dim\"],\n",
    "    hidden_size=config[\"hidden_size\"],\n",
    "    num_layers=config[\"encoder_num_layers\"],\n",
    "    vocab_size=config[\"encoder_vocab_size\"]\n",
    ")\n",
    "\n",
    "decodeur = Decodeur(\n",
    "    emb_dim=config[\"decoder_embedding_dim\"],\n",
    "    hidden_size=config[\"hidden_size\"],\n",
    "    num_layers=config[\"decoder_num_layers\"],\n",
    "    vocab_size=config[\"decoder_vocab_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(encodeur, decodeur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb : torch.Size([32, 9, 100])\n",
      "c : torch.Size([32, 9, 50])\n",
      "z_concat : torch.Size([32, 9, 150])\n",
      "hidden : torch.Size([2, 32, 50])\n",
      "output_concat : torch.Size([32, 9, 200])\n",
      "sequence_preds : torch.Size([32, 9, 10000])\n",
      "preds : torch.Size([32, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_seq = torch.randint(15000, size=(32, 10))\n",
    "target_seq = torch.randint(10000, size=(32, 9))\n",
    "\n",
    "res = seq2seq(source_seq, target_seq)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,280,400 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(seq2seq):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('IA-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "701f8f61e2df02aa7711dfd09ce0ea30c74b88eb04e87f301847efa4bff5d6ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
