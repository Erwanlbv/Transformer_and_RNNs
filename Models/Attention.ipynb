{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la couche d'Attention d'un Transformer \n",
    "\n",
    "- Base : https://github.com/charlesollion/dlexperiments/tree/master/5-Transformers-Intro\n",
    "- *À faire* : Implémenter l'attention de Bahdanau et de Luong (<x, Ay>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. __Q__, __K__ et __V__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 4\n",
    "\n",
    "query_layer = nn.Linear(dim, dim)\n",
    "key_layer = nn.Linear(dim, dim)\n",
    "value_layer = nn.Linear(dim ,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 4]), torch.Size([1, 3, 4]), torch.Size([1, 3, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size = (Batch_size, sequence_length, input_dimension)\n",
    "X = torch.normal(0, 1, size=(1, 3, 4))\n",
    "\n",
    "query = query_layer(X)\n",
    "key = key_layer(X)\n",
    "value = value_layer(X)\n",
    "\n",
    "query.size(), key.size(), value.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$SelfAttention(Q_i, \\mathbf{K}, \\mathbf{V}) = \\sum_j softmax_j(\\frac{Q_i \\cdot \\mathbf{K}^T}{\\sqrt{d_k}}) V_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention d'un Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2109, 0.3170, 0.2986],\n",
       "          [0.5714, 0.3812, 0.4213],\n",
       "          [0.2177, 0.3017, 0.2800]]], grad_fn=<SoftmaxBackward0>),\n",
       " torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "attention_scores = torch.matmul(query, key.mT)\n",
    "attention_scores /= math.sqrt(dim)\n",
    "\n",
    "attention_probs = nn.Softmax(dim=1)(attention_scores)\n",
    "attention_probs, attention_probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2359, -0.1721,  0.0009,  0.4688],\n",
       "         [ 0.2439, -0.2217,  0.0210,  0.4918],\n",
       "         [ 0.2290, -0.1905,  0.0565,  0.4676]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(attention_probs, value) # Somme pondérée avec V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *C'est tout !*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Attention, Normalisation & Prop. avant :\n",
    "(Bloc Encodeur d'un Transformer quoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = hidden_dim\n",
    "        self.query_layer = nn.Linear(self.dim, self.dim)\n",
    "        self.value_layer = nn.Linear(self.dim, self.dim)\n",
    "        self.key_layer = nn.Linear(self.dim, self.dim)\n",
    "\n",
    "        self.output_layer = nn.Linear(self.dim, self.dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.LayerNorm = nn.LayerNorm(self.dim, eps=1e-4)\n",
    "\n",
    "    def self_attention(self, X):\n",
    "        query = self.query_layer(X)\n",
    "        value = self.value_layer(X)\n",
    "        key = self.key_layer(X)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.mT)\n",
    "        attention_scores = attention_scores /  math.sqrt(self.dim)\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=1)(attention_scores)\n",
    "\n",
    "        return torch.matmul(attention_probs, value)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = self.self_attention(X)\n",
    "        Z = self.LayerNorm(X + Z)\n",
    "        Z = self.output_layer(Z)\n",
    "        Z = self.dropout(Z)\n",
    "        return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = SimpleEncoder(hidden_dim=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2061, -0.0000, -0.6968,  0.0000],\n",
       "         [-0.4542, -0.0000, -0.5351, -0.0959],\n",
       "         [ 0.2486, -1.1297,  0.0000, -0.4160]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('IA-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "701f8f61e2df02aa7711dfd09ce0ea30c74b88eb04e87f301847efa4bff5d6ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
